{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_proj.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEk7c_MDbSgX"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZyv_9X0kb6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61430248-c08a-4d2a-aa51-d80a0d68ab4c"
      },
      "source": [
        "!pip install plantcv\n",
        "!pip install c3d"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: plantcv in /usr/local/lib/python3.7/dist-packages (3.11.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from plantcv) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from plantcv) (0.22.2.post1)\n",
            "Requirement already satisfied: dask-jobqueue in /usr/local/lib/python3.7/dist-packages (from plantcv) (0.7.2)\n",
            "Requirement already satisfied: opencv-python<4,>=3.4 in /usr/local/lib/python3.7/dist-packages (from plantcv) (3.4.13.47)\n",
            "Requirement already satisfied: matplotlib>=1.5 in /usr/local/lib/python3.7/dist-packages (from plantcv) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from plantcv) (2.8.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from plantcv) (0.16.2)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "Requirement already satisfied: c3d in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from c3d) (1.19.5)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gnveOQLzZs-"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import c3d\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from plantcv import plantcv as pcv\n",
        "from scipy import ndimage\n",
        "import torchvision"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCAC2zBMNOjE",
        "outputId": "62f6e24b-64fc-476d-ddeb-f41f1ac3439d"
      },
      "source": [
        "## Mount Google Drive Data (If using Google Colaboratory)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "except:\n",
        "    print(\"Mounting Failed.\")\n"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prEI4BySN1tM"
      },
      "source": [
        "data_dir = \"/content/gdrive/MyDrive/BerkeleyMHAD/\"\n",
        "subjects = ['S01', 'S02', 'S03', 'S04', 'S05','S06', 'S07', 'S08', 'S09', 'S10', 'S11', 'S12']\n",
        "actions = ['A01', 'A02', 'A03', 'A04', 'A05','A06', 'A07', 'A08', 'A09', 'A10', 'A11']\n",
        "reps = ['R01', 'R02', 'R03', 'R04', 'R05']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPQyI2ZUbV7Z"
      },
      "source": [
        "#LSTM on Skeleton Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVxw1YsKzHUp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "9c88f588-4b3c-4bcb-cc7f-baeda6329a56"
      },
      "source": [
        "############### Safe to delete this code now. See dataloader below ###################\n",
        "\n",
        "def c3d2array(loc: str):\n",
        "    \"\"\"\n",
        "    Takes in the file location of the c3d file as a string and returns a \n",
        "    numpy array w/ the sensor data\n",
        "    Array Shape: Num_frames x 43 x 3 \n",
        "      43 corresponds to the number of nodes on the person\n",
        "      3 corresponds to spatial coordinates of the nodes\n",
        "    \"\"\"\n",
        "    point_series = []\n",
        "    reader = c3d.Reader(open(loc, 'rb'))\n",
        "    for i, points, analog in reader.read_frames():     \n",
        "        if i % 22 == 0: ## Set Frame Rate to approximately 22 Hz instead of 480 Hz\n",
        "            point_series.append(points[:, 0:3])\n",
        "    point_series = np.array(point_series)\n",
        "    return point_series\n",
        "\n",
        "## Some funny data visualization L0L\n",
        "## the x-coordinate represents the width of the person (shoulder to shouler)\n",
        "## the y-coordinate represents the depth of the person\n",
        "## the z-coordinate represents the height of the person   \n",
        "## There is a point at (0, 0, 0) for all the samples for callibration purposes    \n",
        "test = c3d2array(data_dir + '/Mocap/OpticalData/moc_s01_a02_r01.c3d')\n",
        "frame = 30\n",
        "xs, ys, zs = test[frame, :, 0], test[frame, :, 1], test[frame, :, 2]\n",
        "plt.scatter(xs, zs)\n",
        "plt.axis('scaled')"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-830.5649322509765, 39.55071105957032, -74.84854125976562, 1571.8193664550781)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSGM-xw4cj4_"
      },
      "source": [
        "################ Dataloader for Berkeley Mocap Optical Data #################\n",
        "\n",
        "def c3d2array(loc: str, frame_rate):\n",
        "    \"\"\"\n",
        "    Takes in the file location of the c3d file as a string and returns a \n",
        "    numpy array w/ the sensor data\n",
        "    Array Shape: Num_frames x 43 x 3 \n",
        "      43 corresponds to the number of nodes on the person\n",
        "      3 corresponds to spatial coordinates of the nodes\n",
        "    \"\"\"\n",
        "    point_series = []\n",
        "    reader = c3d.Reader(open(loc, 'rb'))\n",
        "    for i, points, analog in reader.read_frames():     \n",
        "        if i % frame_rate == 0: ## Set Frame Rate to approximately 22 Hz instead of 480 Hz\n",
        "            point_series.append(points[:, 0:3])\n",
        "    point_series = np.array(point_series)\n",
        "    return point_series\n",
        "\n",
        "class BerkeleyMocapOpticalDataset(Dataset):\n",
        "    \"\"\"Berkeley Mocap Optical Dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, frame_rate, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the data\n",
        "            frame_rate (integer): In Hz \n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.list_c3d = [file for file in os.listdir(root_dir) if (os.path.isfile(os.path.join(root_dir, file)) and file[-1] != 't' ) ] #list of all the c3d file names\n",
        "        self.frame_rate = frame_rate\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_c3d)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        c3d_file_location = os.path.join(self.root_dir,\n",
        "                                self.list_c3d[idx])\n",
        "        sample = c3d2array(c3d_file_location, self.frame_rate)\n",
        "        label_location = c3d_file_location.find('_a')\n",
        "        if label_location == -1: # FUCK T-POSE\n",
        "            label = 12\n",
        "        else: \n",
        "            label = int(c3d_file_location[label_location+2:label_location+4])\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        # Reshape Samples, might want to include this in the transform \n",
        "        sample = torch.transpose(sample, 0, 1)\n",
        "        sample = torch.flatten(sample, 1, 2)\n",
        "        sample = torch.unsqueeze(sample, 0)\n",
        "        sample = sample.float()\n",
        "        return sample, label\n",
        "\n",
        "\n",
        "############### Compute Mean and SD across the dataset #############\n",
        "transform = None\n",
        "root_dir = data_dir + 'Mocap/OpticalData'\n",
        "\"\"\"\n",
        "frame_rate = 1\n",
        "\n",
        "dataloader = BerkeleyMocapOpticalDataset(root_dir, 1)\n",
        "x_list = []\n",
        "y_list = []\n",
        "z_list = []\n",
        "\n",
        "for sample in dataloader:\n",
        "  x_list.append(np.ndarray.flatten(sample[:,:,0]))\n",
        "  y_list.append(np.ndarray.flatten(sample[:,:,1]))\n",
        "  z_list.append(np.ndarray.flatten(sample[:,:,2]))\n",
        "\n",
        "x_list = np.concatenate(np.array(x_list))\n",
        "y_list = np.concatenate(np.array(y_list))\n",
        "z_list = np.concatenate(np.array(z_list))\n",
        "x_mean = np.mean(x_list)\n",
        "x_sd = np.std(x_list)\n",
        "y_mean = np.mean(y_list)\n",
        "y_sd = np.std(y_list)\n",
        "z_mean = np.mean(z_list)\n",
        "z_sd = np.std(z_list)\n",
        "\"\"\"\n",
        "\n",
        "############### Define a transform with mean, std, and tensor ############\n",
        "## So we don't need to rerun for the mean and standard deviations every time\n",
        "x_mean, x_sd, y_mean, y_sd, z_mean, z_sd = -459.35840376604654, \n",
        "                                            244.8145592948814, \n",
        "                                            137.52356909040483, \n",
        "                                            179.76890675731673, \n",
        "                                            812.9326420339114, \n",
        "                                            532.8888533425072\n",
        "transform = torchvision.transforms.Compose([\n",
        "  torchvision.transforms.ToTensor(),\n",
        "  torchvision.transforms.Normalize((x_mean, y_mean, z_mean), (x_sd, y_sd, z_sd)),\n",
        "])\n",
        "\n",
        "\n",
        "############# Final Dataloader ####################\n",
        "dataloader = BerkeleyMocapOpticalDataset(root_dir, 22, transform=transform)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ehz48DvAcqVk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "834c6904-d301-4611-d469-af879d9a1928"
      },
      "source": [
        "############ Test Dataloader ######################\n",
        "## Not working right now since I changed the dataloader\n",
        "i = 3\n",
        "for sample in dataloader:\n",
        "  if i == 0:\n",
        "    break\n",
        "  i = i - 1\n",
        "  frame = 1\n",
        "  xs, ys, zs = sample[0, frame, :], sample[1, frame, :], sample[2, frame, :]\n",
        "  plt.figure()\n",
        "  plt.scatter(xs, zs)\n",
        "  plt.axis('scaled')\n",
        "  xs, ys, zs = sample[0, frame+10, :], sample[1, frame+10, :], sample[2, frame+10, :]\n",
        "  plt.scatter(xs, zs)\n",
        "  plt.axis('scaled')"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-135-abe3e681d675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNccTDNGxYdG"
      },
      "source": [
        "class skeletonLSTM(nn.Module):\n",
        "    def __init__(self, classes):\n",
        "        super(skeletonLSTM, self).__init__()\n",
        "        self.n_hidden = 100\n",
        "        self.n_layers = 1\n",
        "        self.l_lstm = torch.nn.LSTM(input_size = 129, \n",
        "                                 hidden_size = self.n_hidden,\n",
        "                                 num_layers = self.n_layers, \n",
        "                                 batch_first = True)\n",
        "        self.fc1 = nn.Linear(100, 50)\n",
        "        self.fc2 = nn.Linear(50, classes)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        #intialize lstm hidden state\n",
        "        hidden_state = torch.zeros(self.n_layers, 1, self.n_hidden)\n",
        "        cell_state = torch.zeros(self.n_layers, 1, self.n_hidden)\n",
        "        self.hidden = (hidden_state, cell_state)      \n",
        "        #print(x.shape)\n",
        "        lstm_out, _ = self.l_lstm(x, self.hidden) #lstm_out shape is batch_size, seq len, hidden state\n",
        "        lstm_out = lstm_out[:,-1,:]\n",
        "        lstm_out = self.relu(self.fc1(lstm_out.squeeze()))\n",
        "        lstm_out = self.fc2(lstm_out)\n",
        "        return lstm_out\n",
        "    "
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYYtqLfyyC0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34894d44-e0c1-47bf-9871-e237c77e27b0"
      },
      "source": [
        " num_epochs = 5\n",
        " num_classes = 12\n",
        " model = skeletonLSTM(12)\n",
        " loss_func = nn.CrossEntropyLoss()\n",
        " optimizer = torch.optim.Adam(model.parameters())\n",
        " \n",
        " for epoch in range(num_epochs):\n",
        "    ########################### Training #####################################\n",
        "    print(\"\\nEPOCH \" +str(epoch+1)+\" of \"+str(num_epochs)+\"\\n\")\n",
        "    for inputs, labels in dataloader:\n",
        "          model.train()\n",
        "          if labels == 12: #FUCK T-POSE\n",
        "              continue\n",
        "          labels = torch.Tensor([labels])\n",
        "          predictions = model(inputs).unsqueeze(0)\n",
        "          loss = loss_func(predictions, labels.long())\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "EPOCH 1 of 5\n",
            "\n",
            "\n",
            "EPOCH 2 of 5\n",
            "\n",
            "\n",
            "EPOCH 3 of 5\n",
            "\n",
            "\n",
            "EPOCH 4 of 5\n",
            "\n",
            "\n",
            "EPOCH 5 of 5\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M57LENMrdCL-",
        "outputId": "548a0245-d130-4a13-abfd-105c004dfdb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## Just looking at the predictions vs the true labels\n",
        "for i in range(600):\n",
        "    sample, label = dataloader[i][0], dataloader[i][1]\n",
        "    print(torch.argmax(model(sample)), label)"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2) 1\n",
            "tensor(5) 1\n",
            "tensor(7) 1\n",
            "tensor(5) 1\n",
            "tensor(7) 1\n",
            "tensor(5) 2\n",
            "tensor(6) 2\n",
            "tensor(1) 2\n",
            "tensor(1) 2\n",
            "tensor(6) 2\n",
            "tensor(6) 3\n",
            "tensor(6) 3\n",
            "tensor(6) 3\n",
            "tensor(6) 3\n",
            "tensor(6) 3\n",
            "tensor(7) 4\n",
            "tensor(7) 4\n",
            "tensor(7) 4\n",
            "tensor(6) 4\n",
            "tensor(7) 4\n",
            "tensor(5) 5\n",
            "tensor(5) 5\n",
            "tensor(5) 5\n",
            "tensor(5) 5\n",
            "tensor(5) 5\n",
            "tensor(6) 6\n",
            "tensor(6) 6\n",
            "tensor(6) 6\n",
            "tensor(6) 6\n",
            "tensor(6) 6\n",
            "tensor(7) 7\n",
            "tensor(7) 7\n",
            "tensor(7) 7\n",
            "tensor(7) 7\n",
            "tensor(7) 7\n",
            "tensor(8) 8\n",
            "tensor(8) 8\n",
            "tensor(8) 8\n",
            "tensor(8) 8\n",
            "tensor(1) 8\n",
            "tensor(9) 9\n",
            "tensor(9) 9\n",
            "tensor(9) 9\n",
            "tensor(9) 9\n",
            "tensor(9) 9\n",
            "tensor(10) 10\n",
            "tensor(10) 10\n",
            "tensor(10) 10\n",
            "tensor(10) 10\n",
            "tensor(10) 10\n",
            "tensor(9) 11\n",
            "tensor(9) 11\n",
            "tensor(9) 11\n",
            "tensor(9) 11\n",
            "tensor(9) 11\n",
            "tensor(1) 1\n",
            "tensor(7) 1\n",
            "tensor(1) 1\n",
            "tensor(1) 1\n",
            "tensor(4) 1\n",
            "tensor(5) 2\n",
            "tensor(5) 2\n",
            "tensor(5) 2\n",
            "tensor(6) 2\n",
            "tensor(5) 2\n",
            "tensor(5) 3\n",
            "tensor(6) 3\n",
            "tensor(5) 3\n",
            "tensor(6) 3\n",
            "tensor(6) 3\n",
            "tensor(4) 4\n",
            "tensor(7) 4\n",
            "tensor(4) 4\n",
            "tensor(4) 4\n",
            "tensor(7) 4\n",
            "tensor(5) 5\n",
            "tensor(5) 5\n",
            "tensor(5) 5\n",
            "tensor(5) 5\n",
            "tensor(5) 5\n",
            "tensor(5) 6\n",
            "tensor(5) 6\n",
            "tensor(5) 6\n",
            "tensor(5) 6\n",
            "tensor(5) 6\n",
            "tensor(7) 7\n",
            "tensor(7) 7\n",
            "tensor(7) 7\n",
            "tensor(7) 7\n",
            "tensor(7) 7\n",
            "tensor(8) 8\n",
            "tensor(8) 8\n",
            "tensor(8) 8\n",
            "tensor(8) 8\n",
            "tensor(8) 8\n",
            "tensor(9) 9\n",
            "tensor(9) 9\n",
            "tensor(9) 9\n",
            "tensor(9) 9\n",
            "tensor(9) 9\n",
            "tensor(10) 10\n",
            "tensor(10) 10\n",
            "tensor(10) 10\n",
            "tensor(10) 10\n",
            "tensor(10) 10\n",
            "tensor(9) 11\n",
            "tensor(9) 11\n",
            "tensor(9) 11\n",
            "tensor(9) 11\n",
            "tensor(9) 11\n",
            "tensor(5) 1\n",
            "tensor(2) 1\n",
            "tensor(2) 1\n",
            "tensor(6) 1\n",
            "tensor(5) 1\n",
            "tensor(5) 2\n",
            "tensor(5) 2\n",
            "tensor(5) 2\n",
            "tensor(5) 2\n",
            "tensor(5) 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-139-a93cbb5ec301>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Just looking at the predictions vs the true labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-134-df8309546c32>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     41\u001b[0m         c3d_file_location = os.path.join(self.root_dir,\n\u001b[1;32m     42\u001b[0m                                 self.list_c3d[idx])\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc3d2array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc3d_file_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mlabel_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc3d_file_location\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel_location\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-134-df8309546c32>\u001b[0m in \u001b[0;36mc3d2array\u001b[0;34m(loc, frame_rate)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpoint_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalog\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mframe_rate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m## Set Frame Rate to approximately 22 Hz instead of 480 Hz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mpoint_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/c3d.py\u001b[0m in \u001b[0;36mread_frames\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# fourth value is floating-point (scaled) error estimate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m             \u001b[0mpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0;31m# fifth value is number of bits set in camera-observation byte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbW1zLV0bbTm"
      },
      "source": [
        "#LSTM on Image Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4kbS8X13n6u"
      },
      "source": [
        "X = []\n",
        "subjects = ['S03', 'S04', 'S05','S06', 'S07', 'S08', 'S09', 'S10', 'S11', 'S12']\n",
        "actions = ['A03', 'A04', 'A05','A06', 'A07', 'A08', 'A09', 'A10', 'A11']\n",
        "for i, action in enumerate(actions):\n",
        "    for subject in subjects:\n",
        "        for repeat in reps:\n",
        "            if subject == 'S04' and action == 'A04' and repeat == 'R05':\n",
        "              continue\n",
        "            if subject == 'S01' or subject == 'S02':\n",
        "              continue\n",
        "            if subject == 'S03' and action == 'A01':\n",
        "              continue\n",
        "            if subject == 'S03' and action == 'A02':\n",
        "              continue\n",
        "            image_path = data_dir + 'Camera/Cluster01/Cam01/' + str(subject) + '/' + str(action) + '/' + str(repeat) + '/'\n",
        "            dirs = os.listdir(image_path)\n",
        "            sample = []\n",
        "            for dir in dirs:\n",
        "                sample.append(ndimage.zoom(pcv.readbayer(image_path + dir, bayerpattern='GR')[0][:,120:480], [1/2, 1/2, 1]))\n",
        "            #X.append(np.array(sample))\n",
        "            np.savez(\"/content/gdrive/My Drive/rgb_video_data/\" + str(subject) + '_' + str(action) + '_' + str(repeat), x=np.array(sample), y=i)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83rvSHOc43q-"
      },
      "source": [
        "class BerkeleyMHAD(Dataset):\n",
        "\n",
        "    def __init__(self, vid_names, root_dir, transform=None):\n",
        "        self.vid_names = vid_names # list of file names for videos (ex. S01_A01_R01)\n",
        "        self.root_dir = root_dir # directory where videos are stored\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vid_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        path = os.path.join(self.root_dir, self.vid_names[idx])\n",
        "        x = np.load(path)['x']\n",
        "        label = np.load(path)['y']\n",
        "        sample = {'x': x, 'y': label}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBsuvtMAOLgg"
      },
      "source": [
        "data_dir = \"/content/gdrive/My Drive/dl_proj_data\"\n",
        "mhad_dataset = BerkeleyMHAD(vid_names=os.listdir(data_dir), root_dir=data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1M2YxAKWGFI",
        "outputId": "58804c0b-1f53-4d71-d9c4-0de7528c8d7e"
      },
      "source": [
        "sample = mhad_dataset[2]\n",
        "sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': array([[[ 17,  17,  17, ...,  54,  48,  55],\n",
              "         [ 16,  20,  20, ...,  51,  38,  48],\n",
              "         [ 25,  23,  23, ...,  55,  52,  50],\n",
              "         ...,\n",
              "         [ 35, 103,  32, ...,  67,  64, 115],\n",
              "         [162, 148, 111, ...,  63,  96, 118],\n",
              "         [ 33, 160,  37, ...,  66,  61, 114]],\n",
              " \n",
              "        [[ 16,  16,  17, ...,  54,  51,  53],\n",
              "         [ 16,  19,  18, ...,  49,  39,  51],\n",
              "         [ 23,  24,  25, ...,  54,  51,  53],\n",
              "         ...,\n",
              "         [ 38, 100,  28, ...,  66,  62, 112],\n",
              "         [157, 149, 109, ...,  63,  93, 121],\n",
              "         [ 33, 163,  36, ...,  64,  58, 112]],\n",
              " \n",
              "        [[ 16,  17,  17, ...,  53,  51,  53],\n",
              "         [ 17,  20,  20, ...,  50,  38,  50],\n",
              "         [ 23,  25,  23, ...,  55,  52,  55],\n",
              "         ...,\n",
              "         [ 36,  97,  31, ...,  69,  60, 115],\n",
              "         [157, 149, 110, ...,  65,  93, 121],\n",
              "         [ 36, 156,  37, ...,  66,  56, 114]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 15,  17,  17, ...,  52,  50,  53],\n",
              "         [ 17,  20,  17, ...,  50,  39,  49],\n",
              "         [ 24,  24,  23, ...,  54,  52,  52],\n",
              "         ...,\n",
              "         [ 34,  99,  29, ...,  68,  64, 113],\n",
              "         [161, 152, 110, ...,  64,  96, 119],\n",
              "         [ 33, 159,  36, ...,  66,  58, 113]],\n",
              " \n",
              "        [[ 15,  17,  17, ...,  51,  52,  52],\n",
              "         [ 16,  20,  18, ...,  49,  36,  48],\n",
              "         [ 25,  25,  23, ...,  53,  52,  52],\n",
              "         ...,\n",
              "         [ 34, 104,  29, ...,  68,  59, 115],\n",
              "         [159, 152, 112, ...,  64,  92, 120],\n",
              "         [ 35, 163,  39, ...,  65,  56, 112]],\n",
              " \n",
              "        [[ 17,  17,  17, ...,  53,  51,  52],\n",
              "         [ 16,  19,  18, ...,  51,  39,  50],\n",
              "         [ 25,  25,  23, ...,  52,  50,  52],\n",
              "         ...,\n",
              "         [ 37, 104,  29, ...,  68,  59, 114],\n",
              "         [164, 153, 111, ...,  66,  94, 120],\n",
              "         [ 35, 160,  36, ...,  67,  60, 111]]], dtype=uint8), 'y': array(0)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    }
  ]
}