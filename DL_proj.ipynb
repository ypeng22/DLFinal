{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_proj.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEk7c_MDbSgX"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZyv_9X0kb6s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8119565-56ab-4aba-b938-35cb93ff2c0e"
      },
      "source": [
        "!pip install plantcv\n",
        "!pip install c3d"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting plantcv\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/26/ed047310781d038fb76d5b5d9cc3ebf61e58ef303d77c692544999accd14/plantcv-3.11.0-py3-none-any.whl (214kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 13.9MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 20.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30kB 25.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 40kB 26.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 51kB 20.3MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 61kB 11.3MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 71kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 81kB 12.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 92kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 102kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 112kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 122kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 133kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 143kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 153kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 163kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 174kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 184kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 194kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 204kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 215kB 13.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from plantcv) (0.16.2)\n",
            "Collecting opencv-python<4,>=3.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/a2/e081e1d8831ce4dc1629ba36f1e010a56225b00532e2edad76e1bc7faef9/opencv_python-3.4.13.47-cp37-cp37m-manylinux2014_x86_64.whl (48.6MB)\n",
            "\u001b[K     |████████████████████████████████| 48.6MB 173kB/s \n",
            "\u001b[?25hRequirement already satisfied: dask in /usr/local/lib/python3.7/dist-packages (from plantcv) (2.12.0)\n",
            "Requirement already satisfied: matplotlib>=1.5 in /usr/local/lib/python3.7/dist-packages (from plantcv) (3.2.2)\n",
            "Requirement already satisfied: plotnine in /usr/local/lib/python3.7/dist-packages (from plantcv) (0.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from plantcv) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from plantcv) (0.22.2.post1)\n",
            "Collecting dask-jobqueue\n",
            "  Downloading https://files.pythonhosted.org/packages/22/4a/bf4ea9c12c825d0926899d9c08d1abc009ac0779c6b2476aa419a02de719/dask_jobqueue-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from plantcv) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from plantcv) (2.8.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from plantcv) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->plantcv) (2.5.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->plantcv) (2.4.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->plantcv) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->plantcv) (1.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->plantcv) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->plantcv) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->plantcv) (0.10.0)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from plotnine->plantcv) (0.5.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from plotnine->plantcv) (0.10.2)\n",
            "Requirement already satisfied: descartes>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from plotnine->plantcv) (1.1.0)\n",
            "Requirement already satisfied: mizani>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from plotnine->plantcv) (0.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->plantcv) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->plantcv) (1.0.1)\n",
            "Collecting distributed>=2.19\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/b0/3454dc44239c526f9c9e4cf04f62823776b71f927db74302986d56e7a9a1/distributed-2021.4.0-py3-none-any.whl (684kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 47.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->plantcv) (1.15.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image->plantcv) (4.4.2)\n",
            "Requirement already satisfied: palettable in /usr/local/lib/python3.7/dist-packages (from mizani>=0.6.0->plotnine->plantcv) (3.3.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (1.7.0)\n",
            "Collecting cloudpickle>=1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/e3/898487e5dbeb612054cf2e0c188463acb358167fef749c53c8bb8918cea1/cloudpickle-1.6.0-py3-none-any.whl\n",
            "Requirement already satisfied: tornado>=5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (5.1.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (3.13)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (1.0.2)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (5.4.8)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (2.3.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (0.11.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (54.2.0)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (2.0.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.19->dask-jobqueue->plantcv) (1.0.1)\n",
            "\u001b[31mERROR: distributed 2021.4.0 has requirement dask>=2021.03.0, but you'll have dask 2.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: dask-jobqueue 0.7.2 has requirement dask>=2.19, but you'll have dask 2.12.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: opencv-python, cloudpickle, distributed, dask-jobqueue, plantcv\n",
            "  Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "  Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "Successfully installed cloudpickle-1.6.0 dask-jobqueue-0.7.2 distributed-2021.4.0 opencv-python-3.4.13.47 plantcv-3.11.0\n",
            "Collecting c3d\n",
            "  Downloading https://files.pythonhosted.org/packages/80/39/1fddfb01d5f142b2ec1c199c78eaca1094f053af66235c76ba4d14fc7950/c3d-0.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from c3d) (1.19.5)\n",
            "Installing collected packages: c3d\n",
            "Successfully installed c3d-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gnveOQLzZs-"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import c3d\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from plantcv import plantcv as pcv\n",
        "from scipy import ndimage\n",
        "import torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCAC2zBMNOjE",
        "outputId": "c0890644-3435-4818-e3f0-d8be7c2178b4"
      },
      "source": [
        "## Mount Google Drive Data (If using Google Colaboratory)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "except:\n",
        "    print(\"Mounting Failed.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prEI4BySN1tM"
      },
      "source": [
        "data_dir = \"/content/gdrive/MyDrive/BerkeleyMHAD/\"\n",
        "subjects = ['S01', 'S02', 'S03', 'S04', 'S05','S06', 'S07', 'S08', 'S09', 'S10', 'S11', 'S12']\n",
        "actions = ['A01', 'A02', 'A03', 'A04', 'A05','A06', 'A07', 'A08', 'A09', 'A10', 'A11']\n",
        "reps = ['R01', 'R02', 'R03', 'R04', 'R05']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPQyI2ZUbV7Z"
      },
      "source": [
        "#LSTM on Skeleton Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVxw1YsKzHUp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "3a864844-3b1f-423c-913b-2cf10bb04481"
      },
      "source": [
        "############### Safe to delete this code now. See dataloader below ###################\n",
        "\n",
        "def c3d2array(loc: str):\n",
        "    \"\"\"\n",
        "    Takes in the file location of the c3d file as a string and returns a \n",
        "    numpy array w/ the sensor data\n",
        "    Array Shape: Num_frames x 43 x 3 \n",
        "      43 corresponds to the number of nodes on the person\n",
        "      3 corresponds to spatial coordinates of the nodes\n",
        "    \"\"\"\n",
        "    point_series = []\n",
        "    reader = c3d.Reader(open(loc, 'rb'))\n",
        "    for i, points, analog in reader.read_frames():     \n",
        "        if i % 22 == 0: ## Set Frame Rate to approximately 22 Hz instead of 480 Hz\n",
        "            point_series.append(points[:, 0:3])\n",
        "    point_series = np.array(point_series)\n",
        "    return point_series\n",
        "\n",
        "## Some funny data visualization L0L\n",
        "## the x-coordinate represents the width of the person (shoulder to shouler)\n",
        "## the y-coordinate represents the depth of the person\n",
        "## the z-coordinate represents the height of the person   \n",
        "## There is a point at (0, 0, 0) for all the samples for callibration purposes    \n",
        "test = c3d2array(data_dir + '/Mocap/OpticalData/moc_s01_a02_r01.c3d')\n",
        "frame = 30\n",
        "xs, ys, zs = test[frame, :, 0], test[frame, :, 1], test[frame, :, 2]\n",
        "plt.scatter(xs, zs)\n",
        "plt.axis('scaled')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-830.5649322509765, 39.55071105957032, -74.84854125976562, 1571.8193664550781)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSGM-xw4cj4_"
      },
      "source": [
        "################ Dataloader for Berkeley Mocap Optical Data #################\n",
        "\n",
        "def c3d2array(loc: str, frame_rate):\n",
        "    \"\"\"\n",
        "    Takes in the file location of the c3d file as a string and returns a \n",
        "    numpy array w/ the sensor data\n",
        "    Array Shape: Num_frames x 43 x 3 \n",
        "      43 corresponds to the number of nodes on the person\n",
        "      3 corresponds to spatial coordinates of the nodes\n",
        "    \"\"\"\n",
        "    point_series = []\n",
        "    reader = c3d.Reader(open(loc, 'rb'))\n",
        "    for i, points, analog in reader.read_frames():     \n",
        "        if i % frame_rate == 0: ## Set Frame Rate to approximately 22 Hz instead of 480 Hz\n",
        "            point_series.append(points[:, 0:3])\n",
        "    point_series = np.array(point_series)\n",
        "    return point_series\n",
        "\n",
        "class BerkeleyMocapOpticalDataset(Dataset):\n",
        "    \"\"\"Berkeley Mocap Optical Dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, frame_rate, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the data\n",
        "            frame_rate (integer): In Hz \n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.list_c3d = [file for file in os.listdir(root_dir) if (os.path.isfile(os.path.join(root_dir, file)) and file[-1] != 't' ) ] #list of all the c3d file names\n",
        "        self.frame_rate = frame_rate\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_c3d)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        c3d_file_location = os.path.join(self.root_dir,\n",
        "                                self.list_c3d[idx])\n",
        "        sample = c3d2array(c3d_file_location, self.frame_rate)\n",
        "        label_location = c3d_file_location.find('_a')\n",
        "        if label_location == -1:\n",
        "            label = 12\n",
        "        else: \n",
        "            label = int(c3d_file_location[label_location+2:label_location+4])\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        # Reshape Samples\n",
        "        sample = torch.transpose(sample, 0, 1)\n",
        "        sample = torch.flatten(sample, 1, 2)\n",
        "        sample = torch.unsqueeze(sample, 0)\n",
        "        sample = sample.float()\n",
        "        return sample, label\n",
        "\n",
        "\n",
        "############### Compute Mean and SD across the dataset #############\n",
        "transform = None\n",
        "root_dir = data_dir + 'Mocap/OpticalData'\n",
        "\"\"\"\n",
        "frame_rate = 1\n",
        "\n",
        "dataloader = BerkeleyMocapOpticalDataset(root_dir, 1)\n",
        "x_list = []\n",
        "y_list = []\n",
        "z_list = []\n",
        "\n",
        "for sample in dataloader:\n",
        "  x_list.append(np.ndarray.flatten(sample[:,:,0]))\n",
        "  y_list.append(np.ndarray.flatten(sample[:,:,1]))\n",
        "  z_list.append(np.ndarray.flatten(sample[:,:,2]))\n",
        "\n",
        "x_list = np.concatenate(np.array(x_list))\n",
        "y_list = np.concatenate(np.array(y_list))\n",
        "z_list = np.concatenate(np.array(z_list))\n",
        "x_mean = np.mean(x_list)\n",
        "x_sd = np.std(x_list)\n",
        "y_mean = np.mean(y_list)\n",
        "y_sd = np.std(y_list)\n",
        "z_mean = np.mean(z_list)\n",
        "z_sd = np.std(z_list)\n",
        "\"\"\"\n",
        "\n",
        "############### Define a transform with mean, std, and tensor ############\n",
        "x_mean, x_sd, y_mean, y_sd, z_mean, z_sd = -459.35840376604654, 244.8145592948814, 137.52356909040483, 179.76890675731673, 812.9326420339114, 532.8888533425072\n",
        "transform = torchvision.transforms.Compose([\n",
        "  torchvision.transforms.ToTensor(),\n",
        "  torchvision.transforms.Normalize((x_mean, y_mean, z_mean), (x_sd, y_sd, z_sd)),\n",
        "])\n",
        "\n",
        "\n",
        "############# Final Dataloader ####################\n",
        "dataloader = BerkeleyMocapOpticalDataset(root_dir, 22, transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ehz48DvAcqVk"
      },
      "source": [
        "############ Test Dataloader ######################\n",
        "i = 3\n",
        "for sample in dataloader:\n",
        "  if i == 0:\n",
        "    break\n",
        "  i = i - 1\n",
        "  frame = 1\n",
        "  xs, ys, zs = sample[0, frame, :], sample[1, frame, :], sample[2, frame, :]\n",
        "  plt.figure()\n",
        "  plt.scatter(xs, zs)\n",
        "  plt.axis('scaled')\n",
        "  xs, ys, zs = sample[0, frame+10, :], sample[1, frame+10, :], sample[2, frame+10, :]\n",
        "  plt.scatter(xs, zs)\n",
        "  plt.axis('scaled')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNccTDNGxYdG"
      },
      "source": [
        "class skeletonLSTM(nn.Module):\n",
        "    def __init__(self, classes):\n",
        "        super(skeletonLSTM, self).__init__()\n",
        "        self.n_hidden = 100\n",
        "        self.n_layers = 1\n",
        "        self.l_lstm = torch.nn.LSTM(input_size = 129, \n",
        "                                 hidden_size = self.n_hidden,\n",
        "                                 num_layers = self.n_layers, \n",
        "                                 batch_first = True)\n",
        "        self.fc1 = nn.Linear(100, 50)\n",
        "        self.fc2 = nn.Linear(50, classes)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        #intialize lstm hidden state\n",
        "        hidden_state = torch.zeros(self.n_layers, 1, self.n_hidden)\n",
        "        cell_state = torch.zeros(self.n_layers, 1, self.n_hidden)\n",
        "        self.hidden = (hidden_state, cell_state)      \n",
        "        #print(x.shape)\n",
        "        lstm_out, _ = self.l_lstm(x, self.hidden) #lstm_out shape is batch_size, seq len, hidden state\n",
        "        lstm_out = lstm_out[:,-1,:]\n",
        "        lstm_out = self.relu(self.fc1(lstm_out.squeeze()))\n",
        "        lstm_out = self.fc2(lstm_out)\n",
        "        return lstm_out\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYYtqLfyyC0Z",
        "outputId": "ac3d9071-ab6e-42c2-b71a-1b4ab04b51fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        " num_epochs = 1\n",
        " num_classes = 12\n",
        " model = skeletonLSTM(12)\n",
        " loss_func = nn.CrossEntropyLoss()\n",
        " optimizer = torch.optim.Adam(model.parameters())\n",
        " \n",
        " for epoch in range(num_epochs):\n",
        "    ########################### Training #####################################\n",
        "    print(\"\\nEPOCH \" +str(epoch+1)+\" of \"+str(num_epochs)+\"\\n\")\n",
        "    for inputs, labels in dataloader:\n",
        "          model.train()\n",
        "          if labels == 12:\n",
        "              continue\n",
        "          labels = torch.Tensor([labels])\n",
        "          predictions = model(inputs).unsqueeze(0)\n",
        "          loss = loss_func(predictions, labels.long())\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "EPOCH 1 of 10\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-e7da1196bb67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEPOCH \"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" of \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0mbatch_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m          \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m          \u001b[0;32mif\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbW1zLV0bbTm"
      },
      "source": [
        "#LSTM on Image Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4kbS8X13n6u"
      },
      "source": [
        "X = []\n",
        "subjects = ['S03', 'S04', 'S05','S06', 'S07', 'S08', 'S09', 'S10', 'S11', 'S12']\n",
        "actions = ['A03', 'A04', 'A05','A06', 'A07', 'A08', 'A09', 'A10', 'A11']\n",
        "for i, action in enumerate(actions):\n",
        "    for subject in subjects:\n",
        "        for repeat in reps:\n",
        "            if subject == 'S04' and action == 'A04' and repeat == 'R05':\n",
        "              continue\n",
        "            if subject == 'S01' or subject == 'S02':\n",
        "              continue\n",
        "            if subject == 'S03' and action == 'A01':\n",
        "              continue\n",
        "            if subject == 'S03' and action == 'A02':\n",
        "              continue\n",
        "            image_path = data_dir + 'Camera/Cluster01/Cam01/' + str(subject) + '/' + str(action) + '/' + str(repeat) + '/'\n",
        "            dirs = os.listdir(image_path)\n",
        "            sample = []\n",
        "            for dir in dirs:\n",
        "                sample.append(ndimage.zoom(pcv.readbayer(image_path + dir, bayerpattern='GR')[0][:,120:480], [1/2, 1/2, 1]))\n",
        "            #X.append(np.array(sample))\n",
        "            np.savez(\"/content/gdrive/My Drive/rgb_video_data/\" + str(subject) + '_' + str(action) + '_' + str(repeat), x=np.array(sample), y=i)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83rvSHOc43q-"
      },
      "source": [
        "class BerkeleyMHAD(Dataset):\n",
        "\n",
        "    def __init__(self, vid_names, root_dir, transform=None):\n",
        "        self.vid_names = vid_names # list of file names for videos (ex. S01_A01_R01)\n",
        "        self.root_dir = root_dir # directory where videos are stored\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vid_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        path = os.path.join(self.root_dir, self.vid_names[idx])\n",
        "        x = np.load(path)['x']\n",
        "        label = np.load(path)['y']\n",
        "        sample = {'x': x, 'y': label}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBsuvtMAOLgg"
      },
      "source": [
        "data_dir = \"/content/gdrive/My Drive/dl_proj_data\"\n",
        "mhad_dataset = BerkeleyMHAD(vid_names=os.listdir(data_dir), root_dir=data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1M2YxAKWGFI",
        "outputId": "58804c0b-1f53-4d71-d9c4-0de7528c8d7e"
      },
      "source": [
        "sample = mhad_dataset[2]\n",
        "sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': array([[[ 17,  17,  17, ...,  54,  48,  55],\n",
              "         [ 16,  20,  20, ...,  51,  38,  48],\n",
              "         [ 25,  23,  23, ...,  55,  52,  50],\n",
              "         ...,\n",
              "         [ 35, 103,  32, ...,  67,  64, 115],\n",
              "         [162, 148, 111, ...,  63,  96, 118],\n",
              "         [ 33, 160,  37, ...,  66,  61, 114]],\n",
              " \n",
              "        [[ 16,  16,  17, ...,  54,  51,  53],\n",
              "         [ 16,  19,  18, ...,  49,  39,  51],\n",
              "         [ 23,  24,  25, ...,  54,  51,  53],\n",
              "         ...,\n",
              "         [ 38, 100,  28, ...,  66,  62, 112],\n",
              "         [157, 149, 109, ...,  63,  93, 121],\n",
              "         [ 33, 163,  36, ...,  64,  58, 112]],\n",
              " \n",
              "        [[ 16,  17,  17, ...,  53,  51,  53],\n",
              "         [ 17,  20,  20, ...,  50,  38,  50],\n",
              "         [ 23,  25,  23, ...,  55,  52,  55],\n",
              "         ...,\n",
              "         [ 36,  97,  31, ...,  69,  60, 115],\n",
              "         [157, 149, 110, ...,  65,  93, 121],\n",
              "         [ 36, 156,  37, ...,  66,  56, 114]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 15,  17,  17, ...,  52,  50,  53],\n",
              "         [ 17,  20,  17, ...,  50,  39,  49],\n",
              "         [ 24,  24,  23, ...,  54,  52,  52],\n",
              "         ...,\n",
              "         [ 34,  99,  29, ...,  68,  64, 113],\n",
              "         [161, 152, 110, ...,  64,  96, 119],\n",
              "         [ 33, 159,  36, ...,  66,  58, 113]],\n",
              " \n",
              "        [[ 15,  17,  17, ...,  51,  52,  52],\n",
              "         [ 16,  20,  18, ...,  49,  36,  48],\n",
              "         [ 25,  25,  23, ...,  53,  52,  52],\n",
              "         ...,\n",
              "         [ 34, 104,  29, ...,  68,  59, 115],\n",
              "         [159, 152, 112, ...,  64,  92, 120],\n",
              "         [ 35, 163,  39, ...,  65,  56, 112]],\n",
              " \n",
              "        [[ 17,  17,  17, ...,  53,  51,  52],\n",
              "         [ 16,  19,  18, ...,  51,  39,  50],\n",
              "         [ 25,  25,  23, ...,  52,  50,  52],\n",
              "         ...,\n",
              "         [ 37, 104,  29, ...,  68,  59, 114],\n",
              "         [164, 153, 111, ...,  66,  94, 120],\n",
              "         [ 35, 160,  36, ...,  67,  60, 111]]], dtype=uint8), 'y': array(0)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    }
  ]
}